Kubernetes on AWS using Kops:
=============================
1. Launch Linux EC2 instance in AWS (Kubernetes Client)

2. Create and attach IAM role to EC2 Instance.Kops need permissions to admin access
	S3
	EC2
	VPC
	Route53
	Autoscaling
	etc..
	
3. Install Kops on EC2

curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x kops-linux-amd64
sudo mv kops-linux-amd64 /usr/local/bin/kops

4. Install kubectl

curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl

5. Create S3 bucket in AWS : S3 bucket is used by kubernetes to persist cluster state, lets create s3 bucket using aws cli 
Note: Make sure you choose bucket name that is uniqe accross all aws accounts

aws s3 mb s3://kops-demo.in.k8s --region ap-south-1

6. Create private hosted zone in AWS Route53

Choose name for example (kops.in)

7 Configure environment variables.
Open .bashrc file

	vi ~/.bashrc
Add following content into .bashrc, you can choose any arbitary name for cluster and make sure buck name matches the one you created in previous step.

export KOPS_CLUSTER_NAME=k8s.in
export KOPS_STATE_STORE=s3://kops-demo.k8s.in
Then running command to reflect variables added to .bashrc

	source ~/.bashrc
8. Create ssh key pair
This keypair is used for ssh into kubernetes cluster

ssh-keygen
	
9. Create a Kubernetes cluster definition.

kops create cluster \
--state=${KOPS_STATE_STORE} \
--node-count=2 \
--master-size=t2.medium \
--node-size=t2.medium \
--zones=us-east-1a,us-east-1b \
--name=${KOPS_CLUSTER_NAME} \
--dns private \
--master-count 1

10. Create kubernetes cluster
kops update cluster --yes --admin
Above command may take some time to create the required infrastructure resources on AWS. Execute the validate command to check its status and wait until the cluster becomes ready

kops validate cluster
For the above above command, you might see validation failed error initially when you create cluster and it is expected behaviour, you have to wait for some more time and check again.

11. To connect to the master
ssh admin@api.kops.in

12. Destroy the kubernetes cluster
kops delete cluster  --yes
Update Nodes and Master in the cluster
We can change numner of nodes and number of masters using following commands

   kops edit ig nodes change minSize and maxSize to 0
   kops get ig- to get master node name
   kops edit ig - change min and max size to 0
   kops update cluster --yes
 
Optional (Create terraform scripts through kops)
  https://github.com/kubernetes/kops/blob/master/docs/terraform.md


Suggestions:
 * validate cluster: kops validate cluster --wait 10m
 * list nodes: kubectl get nodes --show-labels
 * ssh to a control-plane node: ssh -i ~/.ssh/id_rsa ubuntu@
 * the ubuntu user is specific to Ubuntu. If not using Ubuntu please use the appropriate user based on your OS.
 * read about installing addons at: https://kops.sigs.k8s.io/addons.


NAME                  STATUS   ROLES           AGE     VERSION   INTERNAL-IP     EXTERNAL-IP     OS-IMAGE             KERNEL-VERSION    CONTAINER-RUNTIME
i-08042b353c4e63d96   Ready    node            4m10s   v1.26.5   172.20.84.228   3.82.243.106    Ubuntu 20.04.6 LTS   5.15.0-1035-aws   containerd://1.6.18
i-0ac7fc1e8a688f3b8   Ready    control-plane   7m58s   v1.26.5   172.20.44.149   44.195.78.220   Ubuntu 20.04.6 LTS   5.15.0-1035-aws   containerd://1.6.18
i-0e0e5bcb03407b8b7   Ready    node            4m21s   v1.26.5   172.20.41.206   3.91.173.227    Ubuntu 20.04.6 LTS   5.15.0-1035-aws   containerd://1.6.18
